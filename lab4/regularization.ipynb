{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularyzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_fun_multivariable(X, theta):\n",
    "    '''\n",
    "    :param X: ndarray postaci (n+1, m).\n",
    "    :param theta: macierz parametrów do optymalizacji postaci (n+1, 1)\n",
    "    :return: \n",
    "    '''\n",
    "    return np.matmul(theta.T, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeDerivativeMultivariable(X, Y, theta):\n",
    "    '''\n",
    "    :param X: ndarray postaci (n+1, m).\n",
    "    :param y: ndarray z wartościami referencyjnymi o wymiarze (1, m)\n",
    "    :param theta: macierz parametrów do optymalizacji postaci (n+1, 1)\n",
    "    :return: wartość f. kosztu\n",
    "    '''\n",
    "    n,m = X.shape # number of features, number of examples\n",
    "    \n",
    "    diff = (h_fun_multivariable(X, theta) - Y)\n",
    "    gradient = diff @ X.T\n",
    "    gradient = gradient / m\n",
    "\n",
    "    return gradient.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_multivariable(X, y, theta):\n",
    "    '''\n",
    "    :param X: ndarray postaci (n+1, m).\n",
    "    :param y: ndarray z wartościami referencyjnymi o wymiarze (1, m)\n",
    "    :param theta: macierz parametrów do optymalizacji postaci (n+1, 1)\n",
    "    :return: wartość f. kosztu\n",
    "    '''\n",
    "    m = y.shape[1]\n",
    "    value = h_fun_multivariable(X, theta)\n",
    "    return ((value - y) ** 2).sum() / (2*m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_cost_multivariable(np.array([[0, 2, 2, 0]], dtype=np.float64), np.array([[0, 0, 0, 0]], dtype=np.float64), theta = np.array([[1]], dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegression(X, Y, theta_in, learningRate, epsilon):\n",
    "    i = 0\n",
    "    costList = []\n",
    "    derivatveList = []\n",
    "    thetaList = []\n",
    "\n",
    "    theta = deepcopy(theta_in)\n",
    "\n",
    "    while(i < 2 or abs((costList[i-1] - costList[i-2])) > epsilon):\n",
    "        derivative = computeDerivativeMultivariable(X, Y, theta)\n",
    "        derivatveList.append(derivative)\n",
    "\n",
    "        theta = theta - learningRate * derivative\n",
    "        thetaList.append(theta)\n",
    "\n",
    "        cost = calculate_cost_multivariable(X, Y, theta)\n",
    "        costList.append(cost)\n",
    "        \n",
    "        if i < 10 or i % 100000 == 0:\n",
    "            print(f\"Iteration: {i}, Cost: {costList[-1]}\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return [theta, i, costList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeResult(theta, example, X_scale, y_scale):\n",
    "    return np.matmul(theta.T, example/X_scale) * y_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeResultWithScaler(theta, example, x_scaler: StandardScaler):\n",
    "    return np.matmul(theta.T, x_scaler.transform(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./lin_reg_training.csv\") as f:\n",
    "#     csv_reader = csv.reader(f)\n",
    "#     examples = []\n",
    "#     for row in csv_reader:\n",
    "#         examples.append([float(cell) for cell in row])\n",
    "#     data = np.array(examples)\n",
    "#     print(data)\n",
    "#     x = data[:, 0].T\n",
    "#     y = data[:, 1].T\n",
    "#     x = np.reshape(x, [1, -1])\n",
    "#     y = np.reshape(y, [1, -1])\n",
    "#     X = np.concatenate([np.ones([1, x.shape[1]]), x])\n",
    "#     x = x.T\n",
    "#     y = y.T\n",
    "#     X = X.T\n",
    "\n",
    "with open('./lin_reg_training.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "    data_array = np.array(data, dtype=np.float64)\n",
    "    x = data_array[:, 0]\n",
    "    x = np.reshape(x, [1, -1])\n",
    "    X = np.concatenate([np.ones([1, x.shape[1]]), x])\n",
    "    y = data_array[:, 1]\n",
    "    y = np.reshape(y, [1, -1])\n",
    "    x = x.T\n",
    "    y = y.T\n",
    "    X = X.T\n",
    "    print(x)\n",
    "    print(y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_single_dim = x\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(x_train_single_dim)\n",
    "x_train_single_dim = standard_scaler.transform(x_train_single_dim)\n",
    "print(x_train_single_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=6)\n",
    "x_train = poly_features.fit_transform(x_train_single_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.T\n",
    "y_train = y_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_in = np.zeros((x_train.shape[0], 1))\n",
    "eps = 1e-8  # akceptowalna różnica dla kolejnych wartości funkcji kosztu \n",
    "alpha = 0.001  # learning rate\n",
    "theta_0 = 0  # - wartości początkowe parametrów modelu\n",
    "theta_1 = 0\n",
    "\n",
    "theta, i, costList = LinearRegression(x_train, y_train, theta_in, alpha, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x_original = np.reshape(np.linspace(10, 24, 100), [-1, 1])\n",
    "temp_x = standard_scaler.transform(temp_x_original)\n",
    "temp_x = poly_features.transform(temp_x)\n",
    "temp_x = temp_x.T\n",
    "result_ = computeResult(theta, temp_x, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(temp_x_original, result_.T)\n",
    "plt.scatter(x, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = computeResult(theta, x_train, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(standard_scaler.inverse_transform(np.reshape(x_train[1, :], [-1, 1])), result.T)\n",
    "plt.scatter(x, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = calculate_cost_multivariable(x_train, y_train, theta)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./lin_reg_test.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "    data_array = np.array(data, dtype=np.float64)\n",
    "    x_test = data_array[:, 0]\n",
    "    x_test = np.reshape(x_test, [1, -1])\n",
    "    # X = np.concatenate([np.ones([1, x.shape[1]]), x])\n",
    "    y_test = data_array[:, 1]\n",
    "    y_test = np.reshape(y_test, [1, -1])\n",
    "    # x_test = x_test.T\n",
    "    # y_test = y_test.T\n",
    "    # X = X.T\n",
    "    print(x_test)\n",
    "    print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_normalized = standard_scaler.transform(x_test.T)\n",
    "x_test_variable = poly_features.transform(x_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = calculate_cost_multivariable(x_test_variable.T, y_test, theta)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_temp = computeResult(theta, x_test_variable.T, 1, 1)\n",
    "plt.figure()\n",
    "plt.scatter(x_test, y_test, )\n",
    "plt.plot(temp_x_original, result_.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_multivariable_regular(X, y, theta, lambda_):\n",
    "    '''\n",
    "    :param X: ndarray postaci (n+1, m).\n",
    "    :param y: ndarray z wartościami referencyjnymi o wymiarze (1, m)\n",
    "    :param theta: macierz parametrów do optymalizacji postaci (n+1, 1)\n",
    "    :return: wartość f. kosztu\n",
    "    '''\n",
    "    m = y.shape[1]\n",
    "    value = h_fun_multivariable(X, theta)\n",
    "    cost = ((value - y) ** 2).sum() / (2*m)\n",
    "    sum_of_thetas = (theta[1:, :] ** 2).sum()\n",
    "    return cost + sum_of_thetas * lambda_ / (2 * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeDerivativeMultivariable(X, Y, theta):\n",
    "    '''\n",
    "    :param X: ndarray postaci (n+1, m).\n",
    "    :param y: ndarray z wartościami referencyjnymi o wymiarze (1, m)\n",
    "    :param theta: macierz parametrów do optymalizacji postaci (n+1, 1)\n",
    "    :return: wartość f. kosztu\n",
    "    '''\n",
    "    n,m = X.shape # number of features, number of examples\n",
    "    \n",
    "    diff = (h_fun_multivariable(X, theta) - Y)\n",
    "    gradient = np.matmul(diff, X.T)\n",
    "    gradient = gradient / m\n",
    "\n",
    "    return gradient.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegression_regular(X, Y, theta_in, learningRate, epsilon, lambda_):\n",
    "    n, m = X.shape # number of features, number of examples\n",
    "    i = 0\n",
    "    costList = []\n",
    "    derivatveList = []\n",
    "    thetaList = []\n",
    "\n",
    "    theta = deepcopy(theta_in)\n",
    "\n",
    "    while(i < 2 or abs((costList[i-1] - costList[i-2])) > epsilon):\n",
    "        derivative = computeDerivativeMultivariable(X, Y, theta)\n",
    "        derivatveList.append(derivative)\n",
    "\n",
    "        theta_reg = np.concatenate([np.zeros((1, 1)), theta[1:, :]], axis=0)\n",
    "        theta = theta - learningRate * (derivative + lambda_ * theta_reg) / m\n",
    "\n",
    "        thetaList.append(theta)\n",
    "\n",
    "        cost = calculate_cost_multivariable(X, Y, theta)\n",
    "        costList.append(cost)\n",
    "        \n",
    "        # if i < 10 or i % 10000 == 0:\n",
    "        #     print(f\"Iteration: {i}, Cost: {costList[-1]}\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return [theta, i, costList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCostRegular(x_train, y_train, x_test, y_test, lambda_):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    poly_features = PolynomialFeatures(degree=6)\n",
    "\n",
    "    x_train_transformed = scaler.transform(x_train)\n",
    "    x_train_poly = poly_features.fit_transform(x_train_transformed)\n",
    "    x_train_poly = x_train_poly.T\n",
    "\n",
    "    x_data = np.linspace(np.min([np.min(x_train), np.min(x_test)]), np.max([np.max(x_train), np.max(x_test)]), 100)\n",
    "    x_data = np.reshape(x_data, (-1, 1))\n",
    "    x_data_transformed = scaler.transform(x_data)\n",
    "    x_data_poly = poly_features.transform(x_data_transformed)\n",
    "    x_data_poly = x_data_poly.T\n",
    "\n",
    "    x_test_transformed = scaler.transform(x_test)\n",
    "    x_test_poly = poly_features.transform(x_test_transformed)\n",
    "    x_test_poly = x_test_poly.T\n",
    "\n",
    "    theta_in = np.zeros((x_train_poly.shape[0], 1))\n",
    "    eps = 1e-5\n",
    "    alpha = 0.001\n",
    "\n",
    "    theta, i, costList = LinearRegression_regular(x_train_poly, y_train, theta_in, alpha, eps, lambda_)\n",
    "\n",
    "    result_train = computeResult(theta, x_train_poly, 1, 1)\n",
    "    result_data = computeResult(theta, x_data_poly, 1, 1)\n",
    "    result_test = computeResult(theta, x_test_poly, 1, 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(x_train.T, y_train)\n",
    "    plt.plot(x_data, result_data.T)\n",
    "    plt.scatter(x_test.T, y_test)\n",
    "    plt.show()\n",
    "\n",
    "    cost_training = calculate_cost_multivariable_regular(x_train_poly, y_train, theta, 0)\n",
    "    cost_testing = calculate_cost_multivariable_regular(x_test_poly, y_test, theta, 0)\n",
    "\n",
    "    return [cost_training, cost_testing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./lin_reg_training.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "    data_array = np.array(data, dtype=np.float64)\n",
    "    x = data_array[:, 0]\n",
    "    x = np.reshape(x, [1, -1])\n",
    "    y = data_array[:, 1]\n",
    "    y = np.reshape(y, [1, -1])\n",
    "    x_train = x.T\n",
    "    y_train = y\n",
    "    # print(x_train)\n",
    "    # print(y_train)\n",
    "\n",
    "with open('./lin_reg_test.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "    data_array = np.array(data, dtype=np.float64)\n",
    "    x_test = data_array[:, 0]\n",
    "    x_test = np.reshape(x_test, [1, -1])\n",
    "    x_test = x_test.T\n",
    "    y_test = data_array[:, 1]\n",
    "    y_test = np.reshape(y_test, [1, -1])\n",
    "    # print(x_test)\n",
    "    # print(y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_list = [10, 1, 0.01, 0.001, 0.0001]\n",
    "cost_list = []\n",
    "\n",
    "for lambda_ in lambda_list:\n",
    "    print(f\"lambda: {lambda_}\")\n",
    "    cost_train, cost_test = getCostRegular(x_train, y_train, x_test, y_test, lambda_)\n",
    "    cost_list.append((cost_train, cost_test))\n",
    "    print(f\"Train cost: {cost_train}, Test cost: {cost_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array([[2], [3], [4]], dtype=np.float64)\n",
    "aha = np.concatenate([np.zeros((1, 1)), temp[1:, :]], axis=0)\n",
    "print(aha)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regresja Logistyczna"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sigmoid(y):\n",
    "    value_temp =  1.0 / (1.0 + np.exp(-y))\n",
    "    if np.any(value_temp == 1):\n",
    "        value_temp = value_temp - np.finfo(y.dtype).eps\n",
    "    if np.any(value_temp == 0):\n",
    "        value_temp = value_temp + np.finfo(y.dtype).eps\n",
    "    return value_temp"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def h_fun(X, theta):\n",
    "    '''\n",
    "    :param X: ndarray postaci (n+1, m).\n",
    "    :param theta: macierz parametrów do optymalizacji postaci (n+1, 1)\n",
    "    :return: \n",
    "    '''\n",
    "    z = np.matmul(theta.T, X)\n",
    "    h = sigmoid(z)\n",
    "\n",
    "    return h"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_cost(X, y, theta, lambda_):\n",
    "    '''\n",
    "    :param X: ndarray postaci (n+1, m).\n",
    "    :param y: ndarray z wartościami referencyjnymi o wymiarze (1, m)\n",
    "    :param theta: macierz parametrów do optymalizacji postaci (n+1, 1)\n",
    "    :return: wartość f. kosztu\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    h = h_fun(X, theta)\n",
    "    y_1 = -y*np.log(h)\n",
    "    y_0 = -(1-y)*np.log(1-h)\n",
    "    cost_ =  (y_1 + y_0).sum() / m\n",
    "    sum_of_thetas = (theta[1:, :] ** 2).sum()\n",
    "    return cost_ + sum_of_thetas * lambda_ / (2*m)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def computeDerivativeMultivariable(X, Y, theta):\n",
    "    '''\n",
    "    :param X: ndarray postaci (n+1, m).\n",
    "    :param y: ndarray z wartościami referencyjnymi o wymiarze (1, m)\n",
    "    :param theta: macierz parametrów do optymalizacji postaci (n+1, 1)\n",
    "    :return: wartość f. kosztu\n",
    "    '''\n",
    "    n,m = X.shape # number of features, number of examples\n",
    "    \n",
    "    diff = (h_fun(X, theta) - Y)\n",
    "    gradient = np.matmul(diff, X.T)\n",
    "    gradient = gradient / m\n",
    "\n",
    "    return gradient.T"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def LogisticRegression(X, Y, theta_in, learningRate, epsilon, lambda_):\n",
    "    n, m = X.shape # number of features, number of examples\n",
    "    i = 0\n",
    "    costList = []\n",
    "    derivatveList = []\n",
    "    thetaList = []\n",
    "\n",
    "    theta = deepcopy(theta_in)\n",
    "\n",
    "    while(i < 2 or abs((costList[i-1] - costList[i-2])) > epsilon):\n",
    "        derivative = computeDerivativeMultivariable(X, Y, theta)\n",
    "        derivatveList.append(derivative)\n",
    "\n",
    "        theta_reg = np.concatenate([np.zeros((1, 1)), theta[1:, :]], axis=0)\n",
    "        theta = theta - learningRate * (derivative + lambda_ * theta_reg) / m\n",
    "        thetaList.append(theta)\n",
    "\n",
    "        cost = calculate_cost(X, Y, theta, lambda_)\n",
    "        costList.append(cost)\n",
    "        \n",
    "        # if(i < 10 or i % 100000 == 0):\n",
    "        #     print(f\"Iteration: {i}, Cost: {costList[-1]}\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return [theta, i, costList]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def computeResult(theta, example, X_scale, y_scale):\n",
    "    return sigmoid(np.matmul(theta.T, example/X_scale) * y_scale)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('./logi_reg_training.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "    data_array = np.array(data, dtype=np.float64)\n",
    "    x = data_array[:, 0:2]\n",
    "    y = data_array[:, 2]\n",
    "    x_train = x\n",
    "    y_train = y\n",
    "\n",
    "with open('./logi_reg_test.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "    data_array = np.array(data, dtype=np.float64)\n",
    "    x_test = data_array[:, 0:2]\n",
    "    y_test = data_array[:, 2]\n",
    "    x_test = x_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def getCostRegular(x_train, y_train, x_test, y_test, lambda_):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    poly_features = PolynomialFeatures(degree=6)\n",
    "\n",
    "    x_train_transformed = scaler.transform(x_train)\n",
    "    x_train_poly = poly_features.fit_transform(x_train_transformed)\n",
    "    x_train_poly = x_train_poly.T\n",
    "\n",
    "    # x_data = np.linspace(np.min([np.min(x_train), np.min(x_test)]), np.max([np.max(x_train), np.max(x_test)]), 100)\n",
    "    # x_data = np.reshape(x_data, (-1, 1))\n",
    "    # x_data_transformed = scaler.transform(x_data)\n",
    "    # x_data_poly = poly_features.transform(x_data_transformed)\n",
    "    # x_data_poly = x_data_poly.T\n",
    "\n",
    "    x_test_transformed = scaler.transform(x_test)\n",
    "    x_test_poly = poly_features.transform(x_test_transformed)\n",
    "    x_test_poly = x_test_poly.T\n",
    "\n",
    "    theta_in = np.zeros((x_train_poly.shape[0], 1))\n",
    "    eps = 1e-7\n",
    "    alpha = 0.001\n",
    "\n",
    "    theta, i, costList = LogisticRegression(x_train_poly, y_train, theta_in, alpha, eps, lambda_)\n",
    "\n",
    "    result_train = computeResult(theta, x_train_poly, 1, 1)\n",
    "    # result_data = computeResult(theta, x_data_poly, 1, 1)\n",
    "    result_test = computeResult(theta, x_test_poly, 1, 1)\n",
    "    \n",
    "    # logistic regression\n",
    "    value0_train = x_train[np.where(y_train == 0), :]\n",
    "    value1_train = x_train[np.where(y_train == 1), :]\n",
    "    value0_train = np.reshape(value0_train, (-1, 2))\n",
    "    value1_train = np.reshape(value1_train, (-1, 2))\n",
    "    \n",
    "    value0_test = x_test[np.where(y_test == 0), :]\n",
    "    value1_test = x_test[np.where(y_test == 1), :]\n",
    "    value0_test = np.reshape(value0_test, (-1, 2))\n",
    "    value1_test = np.reshape(value1_test, (-1, 2))\n",
    "    \n",
    "    ax = plt.figure()\n",
    "    plt.scatter(value0_train[:, 0], value0_train[:, 1], marker='o')\n",
    "    plt.scatter(value1_train[:, 0], value1_train[:, 1], marker='x')\n",
    "\n",
    "    plt.scatter(value0_test[:, 0], value0_test[:, 1], marker='.')\n",
    "    plt.scatter(value1_test[:, 0], value1_test[:, 1], marker='*')\n",
    "    \n",
    "    x1 = x_train[:, 0]\n",
    "    x2 = x_train[:, 1]\n",
    "    x1_range = np.linspace(x1.min(), x1.max(), 100)\n",
    "    x2_range = np.linspace(x2.min(), x2.max(), 100)\n",
    "    x1x1, x2x2 = np.meshgrid(x1_range, x2_range)\n",
    "    points = np.c_[x1x1.ravel(), x2x2.ravel()]\n",
    "    x1x2_range_comb = scaler.transform(points)\n",
    "    x1x2_range_comb = poly_features.transform(x1x2_range_comb)\n",
    "    X_range_comb = x1x2_range_comb.T\n",
    "    h_range_comb = h_fun(X_range_comb, theta)\n",
    "    h_range_comb.shape = x1x1.shape\n",
    "    plt.contour(x1x1, x2x2, h_range_comb, levels=[0.5], colors='g')\n",
    "    \n",
    "    ax.legend([\"does not belong to a class - train\", \"belongs to a class - training\", \"does not belong to a class - test\", \"belong to a class - test\"])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # plt.figure()\n",
    "    # plt.scatter(x_train.T, y_train)\n",
    "    # plt.plot(x_data, result_data.T)\n",
    "    # plt.scatter(x_test.T, y_test)\n",
    "    # plt.show()\n",
    "\n",
    "    cost_training = calculate_cost(x_train_poly, y_train, theta, 0)\n",
    "    cost_testing = calculate_cost(x_test_poly, y_test, theta, 0)\n",
    "\n",
    "    return [cost_training, cost_testing]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lambda_ = [10, 1, 0.5, 0.1, 0.05, 0.01, 0]\n",
    "cost_list = []\n",
    "\n",
    "for lambda_ in lambda_list:\n",
    "    print(f\"lambda: {lambda_}\")\n",
    "    cost_train, cost_test = getCostRegular(x_train, y_train, x_test, y_test, lambda_)\n",
    "    cost_list.append((cost_train, cost_test))\n",
    "    print(f\"Train cost: {cost_train}, Test cost: {cost_test}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression Scikit Learn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    " \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "x = data.data\n",
    "y = data.target\n",
    " \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=12345)\n",
    "standard_scaler = StandardScaler()\n",
    "x_train = standard_scaler.fit_transform(x_train)\n",
    "x_test = standard_scaler.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "log_regr = LogisticRegression(penalty=None)\n",
    "log_regr.fit(x_train, y_train)\n",
    "y_train_probab = log_regr.predict_proba(x_train)\n",
    "y_test_probabs = log_regr.predict_proba(x_test)\n",
    "cost_train = log_loss(y_train, y_train_probab)\n",
    "cost_test = log_loss(y_test, y_test_probabs)\n",
    "y_test_pred = log_regr.predict(x_test)\n",
    "report = classification_report(y_test, y_test_pred)\n",
    "print(f\"Train cost: {cost_train}\")\n",
    "print(f\"Test cost: {cost_test}\")\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "x = data.data\n",
    "y = data.target\n",
    " \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=12345)\n",
    "standard_scaler = StandardScaler()\n",
    "x_train = standard_scaler.fit_transform(x_train)\n",
    "x_test = standard_scaler.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "poly_features = PolynomialFeatures(degree=6)\n",
    "x_train = poly_features.fit_transform(x_train)\n",
    "x_test = poly_features.transform(x_test)\n",
    "\n",
    "log_regr = LogisticRegression(penalty=None)\n",
    "log_regr.fit(x_train, y_train)\n",
    "y_train_probab = log_regr.predict_proba(x_train)\n",
    "y_test_probabs = log_regr.predict_proba(x_test)\n",
    "cost_train = log_loss(y_train, y_train_probab)\n",
    "cost_test = log_loss(y_test, y_test_probabs)\n",
    "y_test_pred = log_regr.predict(x_test)\n",
    "report = classification_report(y_test, y_test_pred)\n",
    "print(f\"Train cost: {cost_train}\")\n",
    "print(f\"Test cost: {cost_test}\")\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lambda_list = [100, 10, 1, 0.1, 0.01]\n",
    "\n",
    "for lambda_ in lambda_list:\n",
    "    data = load_iris()\n",
    "    x = data.data\n",
    "    y = data.target\n",
    "     \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=12345)\n",
    "    standard_scaler = StandardScaler()\n",
    "    x_train = standard_scaler.fit_transform(x_train)\n",
    "    x_test = standard_scaler.transform(x_test)\n",
    "    \n",
    "    poly_features = PolynomialFeatures(degree=6)\n",
    "    x_train = poly_features.fit_transform(x_train)\n",
    "    x_test = poly_features.transform(x_test)\n",
    "    \n",
    "    log_regr = LogisticRegression(penalty=\"l2\", n_jobs = -1, C = lambda_, max_iter = 10000)\n",
    "    log_regr.fit(x_train, y_train)\n",
    "    y_train_probab = log_regr.predict_proba(x_train)\n",
    "    y_test_probabs = log_regr.predict_proba(x_test)\n",
    "    cost_train = log_loss(y_train, y_train_probab)\n",
    "    cost_test = log_loss(y_test, y_test_probabs)\n",
    "    y_test_pred = log_regr.predict(x_test)\n",
    "    report = classification_report(y_test, y_test_pred)\n",
    "    print(f\"lambda: {lambda_}\")\n",
    "    print(f\"Train cost: {cost_train}\")\n",
    "    print(f\"Test cost: {cost_test}\")\n",
    "    print(report)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
